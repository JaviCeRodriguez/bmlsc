{"cells":[{"cell_type":"markdown","id":"a2cd055b-c7c6-4f94-a5e5-032df78f3c24","metadata":{"id":"a2cd055b-c7c6-4f94-a5e5-032df78f3c24"},"source":["<font size=8>Reducción dimensional</font>"]},{"cell_type":"code","execution_count":null,"id":"471e01fa-f9e0-4db1-b277-459eaf763cc5","metadata":{"id":"471e01fa-f9e0-4db1-b277-459eaf763cc5","tags":[]},"outputs":[],"source":["# Para que ande tanto en python 2 cuanto en python 3\n","from __future__ import division, print_function, unicode_literals\n","\n","# Importaciones comunes\n","import numpy as np\n","import os, sys\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# Para que la salida de este notebook sea la misma en distintas ejecuciones\n","np.random.seed(42)\n","\n","# Para hacer lindas gráficas \n","import matplotlib as mpl\n","mpl.rc('axes', labelsize=16)\n","mpl.rc('xtick', labelsize=14)\n","mpl.rc('ytick', labelsize=14)\n","\n","# Donde grabar las figuras\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"11_Unsupervised\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    os.makedirs(IMAGES_PATH, exist_ok=True)\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)\n","\n","# Ignorar advertencias (warnings) inútiles (ver el SciPy issue #5998)\n","# import warnings\n","# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"]},{"cell_type":"markdown","id":"violent-stone","metadata":{"id":"violent-stone","tags":[]},"source":["\n","# Reducción de dimensionalidad"]},{"cell_type":"markdown","id":"sharp-gospel","metadata":{"id":"sharp-gospel"},"source":["Trabajar en espacios de gran dimensionalidad es frecuentemente inconveniente o incluso imposible. Uno puede querer reducir la dimensionalidad con vistas a:\n","* hacer visualizaciones\n","* ahorrar espacio ocupado por los datos (compresión)\n","* reducir los costos computationales de entrenamiento de algoritmos, reduciendo el número de características\n","* aumentar la performance de un modelo aumentando la señal/ruído, así como la densidad de puntos (*evitar la maldición de la dimensionalidad*).\n","\n","Al problema de reducir la dimensionalidad de las características, mientras se mantiene la mayor parte de la información se lo conoce como *reducción de la dimensionalidad* y hay dos enfoques principales para atacarlo:\n","* Proyección: proyectar el espacio de gran dimensionalidad en un hiper-plano, colapsando las características ortogonales a él\n","* Aprendizaje de variedad (*Manifold-Learning*): aprender la geometría de una variedad de menor dimensionalidad y proyectar los datos en ella\n","\n","En la primera categoría, el algoritmo más conocido es el de Análisis de Componentes Principales."]},{"cell_type":"markdown","id":"potential-diamond","metadata":{"id":"potential-diamond","tags":[]},"source":["## Análisis de Componentes Principales - *Principal Component Analysis*"]},{"cell_type":"markdown","id":"inside-solid","metadata":{"id":"inside-solid"},"source":["El objetivo del Análisis de Componente Principales es encontrar las direcciones sobre las cuales proyectar los datos de forma a minimizar la pérdida de infromación al reducir la dimensionalidad.\n","\n","Veremos eso en acción, pero antes démosle una mirada más intuitiva e ingenua al problema, a partir de un ejemplo simplificado."]},{"cell_type":"markdown","id":"c7a7be40-6431-434a-a555-6bbdd1c0074c","metadata":{"id":"c7a7be40-6431-434a-a555-6bbdd1c0074c"},"source":["### Enfoque ingenuo a la reducción dimensional\n"]},{"cell_type":"markdown","id":"405cfae1-f5ad-4eeb-9d06-a930e92c8bd4","metadata":{"id":"405cfae1-f5ad-4eeb-9d06-a930e92c8bd4"},"source":["Vamos a crear un conjunto de datos fictício, $\\{x^{(i)}\\}$, de puntos en 2 dimensiones (2D) y vamos a suponer que, por alguna razón, queremos reducir los datos a una única dimensión. O sea, cada instancia de datos $x^{(i)}$ es representada por dos números reales ($x^{(i)} \\in \\mathbb{R}^2$, en la jerga matemática), y queremos construir un conjunto aproximado da datos en un espacio unidimensional ($z^{(i)} \\in \\mathbb{R}$), o sea, cada nueva instancia $z^{(i)}$ será representada por un único número. "]},{"cell_type":"markdown","id":"527e764b-6c23-41d7-a521-6830099f2195","metadata":{"id":"527e764b-6c23-41d7-a521-6830099f2195"},"source":["Evidentemente, salvo en situaciones muy espacíficas (por ejemplo, $x_2 = 2 * x_1$) vamos a perder alguna información haciendo eso. El juego es encontrar la forma de elegir ese único número que reduzca esa perdida de información.\n","\n","Empecemos."]},{"cell_type":"markdown","id":"0920549a-1cfd-48dc-8dfa-81d20ad8a431","metadata":{"id":"0920549a-1cfd-48dc-8dfa-81d20ad8a431"},"source":["**Crear un conjunto de datos 2D a partir de una distribución normal homognenea y hacerla rotar**"]},{"cell_type":"code","execution_count":null,"id":"ebb83cb3-bacc-4306-95b0-e981dc476bc1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1661205514696,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"ebb83cb3-bacc-4306-95b0-e981dc476bc1","outputId":"eae31fff-9a9b-4fb6-8ec8-4e1bb2a19a2d"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Crear dats independientes con distribución gaussiana (200 puntos)\n","m = 200\n","np.random.seed(3)\n","X_base = np.random.randn(m, 2) \n","\n","# Verificar la dimensión del array (matriz) resultante\n","print(X_base.shape)"]},{"cell_type":"code","execution_count":null,"id":"996ff08c-198c-48f1-854c-d8851b40a261","metadata":{"id":"996ff08c-198c-48f1-854c-d8851b40a261","tags":[]},"outputs":[],"source":["# Definir el angulo por el cual vamos a rotar los datos y el factor por el cual los estiraremos\n","angle = np.pi / 8\n","stretch = 5\n","\n","# Matriz de estiramiento (deformación)\n","S = np.array([[stretch, 0],\n","              [0, 1]])\n","\n","# Matriz de rotación\n","R = np.array([[np.cos(angle), np.sin(angle)], \n","              [-np.sin(angle), np.cos(angle)]])\n","\n","# Estiramiento\n","X = X_base @ S\n","# Rotación\n","X = X @ R"]},{"cell_type":"code","execution_count":null,"id":"920ce30c-3188-4680-a02b-ca055126d410","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1661205523017,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"920ce30c-3188-4680-a02b-ca055126d410","outputId":"9fb262b0-9fbc-47ae-cdea-b59e405ac492"},"outputs":[],"source":["# Miremos el conjunto de datos resultante\n","ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n","ax.plot(*X.T, marker='x', color='k', ls='')\n","ax.set_aspect('equal')\n","ax.set_xlabel('$X_1$', fontsize=16)\n","ax.set_ylabel('$X_2$', fontsize=16, rotation=0)"]},{"cell_type":"markdown","id":"2497e700-203f-40e3-a3ae-b7edb60ce4f9","metadata":{"id":"2497e700-203f-40e3-a3ae-b7edb60ce4f9"},"source":["Evidentemente, esos son solo números aleatorios en 2D. Vamos a suponer que centramos los datos sustrayendo el promedio de cada eje (por eso los datos están centrados en 0.0).\n","\n","Pueden pensar en un par de ejemplos de conjuntos de datos donde esa situación puede existir:\n","\n","* altura versus peso de una muestra de personas para una dada población\n","* precio de algún bien versus la demanda por ese bien\n","\n","**¿Pueden pensar en uno o dos ejemplos adicionales?**"]},{"cell_type":"markdown","id":"temporal-anxiety","metadata":{"id":"temporal-anxiety"},"source":["Una forma de reducir la dimensionalidad de ese conjunto de datos sería simplemente **quitar una de las dimensiones**. "]},{"cell_type":"markdown","id":"77c69347","metadata":{"id":"77c69347"},"source":["### Descartando una dimensión"]},{"cell_type":"markdown","id":"5f06389c","metadata":{"id":"5f06389c"},"source":["Podemos ver los histogramas de la proyección de los datos en $x$ e $y$ "]},{"cell_type":"code","execution_count":null,"id":"-r1ZRUbiXFxS","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1661205584045,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"-r1ZRUbiXFxS","outputId":"f1d36b74-44d6-4e24-f77b-7290c7e6812c"},"outputs":[],"source":["ax = plt.figure(figsize=(10, 5)).add_subplot(111)\n","\n","ax.hist(X[:, 0], label='$x_1$', color='orange', bins=25)\n","ax.hist(X[:,1], label='$x_2$', color='lightblue', bins=25)\n","ax.legend(loc=0, fontsize=17)\n"]},{"cell_type":"markdown","id":"1b65e8a3-93ea-4e25-9dcb-39e4eba42cb2","metadata":{"id":"1b65e8a3-93ea-4e25-9dcb-39e4eba42cb2"},"source":["Claramente **ninguna de esas dos direcciones** es buena.\n","\n","**¿Pueden imaginar las direcciones que piensan que producirían una mejor conservación de la información de los datos? ¿Por qué?**"]},{"cell_type":"markdown","id":"dcbca033","metadata":{"id":"dcbca033"},"source":["### *Proyección en las diagonales"]},{"cell_type":"markdown","id":"e5c58cae","metadata":{"id":"e5c58cae"},"source":["Mirando a los datos, parece tener sentido **proyectarlos en las direcciones diagonales** y quedarse con solo una de ellas. Hay dos diagonales, entonces hagamos las dos proyecciones y veamos cual funcional mejor."]},{"cell_type":"code","execution_count":null,"id":"steady-treasury","metadata":{"id":"steady-treasury"},"outputs":[],"source":["variant_1 = X[:, 0] + X[:, 1] # x + y\n","variant_2 = X[:, 0] - X[:, 1] # x - y"]},{"cell_type":"code","execution_count":null,"id":"274ba83a-267d-459e-824d-c3cc810814e8","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1661198071932,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"274ba83a-267d-459e-824d-c3cc810814e8","outputId":"5e61fb0e-223c-4a9f-e0c6-6cb499433fc5"},"outputs":[],"source":["# Miremos los conjuntos de datos resultantes \n","ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n","ax.plot(*X.T, marker='x', color='k', ls='')\n","ax.set_aspect('equal')\n","\n","imin = np.argmin(X[:, 0])\n","imax = np.argmax(X[:, 0])\n","\n","ax.plot([X[imin, 0], X[imax, 0]], [X[imin, 0], X[imax, 0]], ls=':', color='gray')\n","ax.plot([X[imin, 0], X[imax, 0]], [-X[imin, 0], -X[imax, 0]], ls=':', color='gray')\n","\n","# Annotate new axes\n","ax.annotate('$z_1$', [X[imax, 0], X[imax, 0]], fontsize=16)\n","ax.annotate('$z_2$', [X[imax, 0], -X[imax, 0]], fontsize=16)\n","\n","# Saque los comentários en estas dos lineas para hacer gráficas de las proyecciones\n","# ax.plot(0.5*variant_1, 0.5*variant_1, 'x', color='orange', ls='')\n","# ax.plot(0.5*variant_2, -0.5*variant_2, 'x', color='lightblue', ls='')\n","\n","# Labels\n","ax.set_xlabel('$X_1$', fontsize=16)\n","ax.set_ylabel('$X_2$', fontsize=16, rotation=0)"]},{"cell_type":"markdown","id":"M91GIow-IAHq","metadata":{"id":"M91GIow-IAHq"},"source":["Ahora veamos como quedan los histogramas de la proyección de los datos en las diagonales ($z_1$ y $z_2$) "]},{"cell_type":"code","execution_count":null,"id":"intelligent-lincoln","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1661198091059,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"intelligent-lincoln","outputId":"a02dd344-b41b-407d-dd81-00ea158b8ac5"},"outputs":[],"source":["ax = plt.figure(figsize=(10, 5)).add_subplot(111)\n","\n","ax.hist(variant_1, label='$z_1$', color='orange', bins=25)\n","ax.hist(variant_2, label='$z_2$', color='lightblue', bins=25)\n","ax.legend(loc=0, fontsize=17)\n","\n","ax.set_xlabel('$z$', fontsize=16)"]},{"cell_type":"markdown","id":"494e8eb4-9d34-4138-bbc0-4774bda647ac","metadata":{"id":"494e8eb4-9d34-4138-bbc0-4774bda647ac"},"source":["Vemos que hay una dirección en que los datos tienen más dispersión que en la otra y probablemente preserve más información. \n","Esa sería una buena candidata para escoger como $z$.\n","\n","**¿Cual de las dos opciones elegirían?**"]},{"cell_type":"markdown","id":"82ec60dd-80ae-4611-94f7-e12148757c19","metadata":{"id":"82ec60dd-80ae-4611-94f7-e12148757c19"},"source":["### Ejecutando PCA"]},{"cell_type":"markdown","id":"moved-thinking","metadata":{"id":"moved-thinking"},"source":["La principal finalidad del Análisis de Componentes Principales, PCA (de *Principal Component Analysis*) es encontrar el conjunto óptimo de ejes sobre los cuales proyectar los datos disponibles. Cada una de las direcciones de esos ejes es llamada de Componente Principal.\n","\n","En PCA la elección de los ejes más importantes se hace según su varianza. La primera Componente Principal es la dirección que continene la mayor parte de la varianza de los datos. Otras direcciones son encontradas (tantas cuanto el número de dimensiones, o sea, características, en el conjunto de datos), con la condición de que cada componente sucesiva retenga la mayor parte posible de la varianza que queda y que sea ortogonal a todas las anteriores. \n","\n","En otras palabras, el algoritmo ajusta un elipsoide a los datos y entonces rota los ejes para que estén alineados con los ejes principales del elipsoide. Los ejes rotados son las componentes principales. Para las personas que les gusten el álgebra lineal, esto es equivalente a encontrar una base ortogonal en la que la matriz de covarianza es diagonalizada. "]},{"cell_type":"markdown","id":"afbea698-cd25-4547-973e-7042689c7fbe","metadata":{"id":"afbea698-cd25-4547-973e-7042689c7fbe"},"source":["Veamos la implementación en `sklearn`.\n","\n","En este caso estamos lidiando con una clase conocida como `Transformer` (no confundir con los modelos de *Deep Learning* de mismo nombre que están en voga en la actualidad). En `sklearn`, esas clases tienen un método `.fit` que toman un conjunto de datos como argumento y aprende algunos parámetros a partir de él. En este caso, el método aprende la matriz de covarianza a partir de los datos y como invertirla. También tienen un método `transform` que usa los parámetros aprendidos para hacer una transformación en el conjunto de datos. En este caso, convierte el conjunto de datos original en el conjunto reducido.\n","\n","Finalmente, el método `fit_transform` concatena los dos pasos.\n","\n","Es importante para hacer PCA que el conjunto de datos esté normalizado. Para eso usaremos otro *transformer* que ya hemos visto, el `StandardScaler`."]},{"cell_type":"code","execution_count":null,"id":"canadian-figure","metadata":{"id":"canadian-figure"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Instanciar el escalonador y escalonar los datos\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Encuentra los componentes principales (cuantos componentes queremos?)\n","pca = PCA(n_components=1)\n","X_pca = pca.fit_transform(X_scaled)"]},{"cell_type":"markdown","id":"39821134-8c89-4f28-a548-3682a61e0661","metadata":{"id":"39821134-8c89-4f28-a548-3682a61e0661"},"source":["**Exploren la forma de la salida. ¿Tiene sentido?**"]},{"cell_type":"code","execution_count":null,"id":"559655b8-7e49-4450-83f6-4e7bdd5083e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1661205820292,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"559655b8-7e49-4450-83f6-4e7bdd5083e2","outputId":"e5ba4b8f-f4f8-4823-e9c3-5c66eb6e7c8d"},"outputs":[],"source":["print(X.shape, X_pca.shape)"]},{"cell_type":"markdown","id":"typical-china","metadata":{"id":"typical-china"},"source":["Las coordenadas de los ejes elegidos para proyectar (o sea, los componentes principales) son guardados en el atributo `components_` en cada línea. "]},{"cell_type":"code","execution_count":null,"id":"7ce7acba-e6b8-4acf-adb7-981760b7efb8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1661205877386,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"7ce7acba-e6b8-4acf-adb7-981760b7efb8","outputId":"5d389530-fc13-4c2d-b7e6-5284f6c851f6"},"outputs":[],"source":["pca.components_.shape"]},{"cell_type":"markdown","id":"30124917-e979-437a-8c68-3ccafc39b224","metadata":{"id":"30124917-e979-437a-8c68-3ccafc39b224"},"source":["**Veamos esas direcciones en la gráfica de arriba**"]},{"cell_type":"code","execution_count":null,"id":"41b448a7-0f92-4174-8e9e-fca135ec7888","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"elapsed":1956,"status":"ok","timestamp":1661205892735,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"41b448a7-0f92-4174-8e9e-fca135ec7888","outputId":"be437224-2514-481d-b372-032f5cffcd82"},"outputs":[],"source":["# Miremos el conjunto de datos resultante\n","ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n","ax.plot(*X_scaled.T, marker='x', color='k', ls='')\n","ax.set_aspect('equal')\n","\n","z = pca.components_\n","\n","for i in range(len(z)):\n","    #\n","    # dirección del componente (inclinación)\n","    m = z[i, 0] / z[i, 1]\n","    \n","    # Graficar línea\n","    ax.plot([X_scaled[imin, 0], X_scaled[imax, 0]], \n","            [X_scaled[imin, 0] * m, X_scaled[imax, 0] * m], \n","            ls=':', color='gray')\n","    \n","    # Graficar flechas\n","    size=1.0\n","    ax.arrow(0, 0, z[i, 0]*size, z[i,1]*size, color='C{}'.format(i+1), width=0.05, head_width=0.1, alpha=0.8,\n","             label='Primer componente principal')\n","    \n","    # Anotar nuevos ejes\n","    ax.annotate('$z_{}$'.format(i+1), [X_scaled[imin, 0], X_scaled[imin, 0]*m], fontsize=16, color='C{}'.format(i+1))\n","\n","# Quitar los comentários a estas líneas para graficar proyecctiones en la primer Componente Principal (definir como cero en el segundo componente)\n","# X_pca_cut = X_pca.copy()\n","# X_pca_cut[:,1]=0\n","# X_1 = pca.inverse_transform(X_pca_cut)\n","# ax.plot(*X_1.T, 'x', color='orange', ls='')\n","\n","# Labels\n","ax.set_xlabel('$X_1$', fontsize=16)\n","ax.set_ylabel('$X_2$', fontsize=16, rotation=0)"]},{"cell_type":"markdown","id":"30f53729-b076-44b4-a0fc-fa0496b96b50","metadata":{"heading_collapsed":"true","id":"30f53729-b076-44b4-a0fc-fa0496b96b50","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### *Un poquito de álgebra"]},{"cell_type":"markdown","id":"8995173c-f7e6-45a0-896e-3d18a49aceb6","metadata":{"id":"8995173c-f7e6-45a0-896e-3d18a49aceb6"},"source":["Calculemos (numericamente) la matriz de covarianza del conjunto original de datos"]},{"cell_type":"markdown","id":"trained-grant","metadata":{"id":"trained-grant"},"source":["Ahora miremos a la matriz de covarianza de las coordenadas transformadas."]},{"cell_type":"code","execution_count":null,"id":"elder-offering","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1661206009867,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"elder-offering","outputId":"16e0c073-6e93-4e16-d131-7925bb25204e"},"outputs":[],"source":["# Encuentra las componentes principales (cuantas componentes queremos?)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","print(np.cov(X_pca.T).round(2))"]},{"cell_type":"markdown","id":"bizarre-century","metadata":{"id":"bizarre-century"},"source":["Noten dos cosas:\n","1. la matriz de covarianza es ahora diagonal\n","2. la mayor parte de la varianza está en la primera coordenada (PCA ordena automaticamente las características por su varianza)."]},{"cell_type":"markdown","id":"930087ca-95d1-4780-b4af-e37f41966834","metadata":{"id":"930087ca-95d1-4780-b4af-e37f41966834","tags":[]},"source":["### Datos en los ejes transformados"]},{"cell_type":"code","execution_count":null,"id":"d3c63b3a-cfd1-4996-b456-9fc35fcd16e1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1661206084884,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"d3c63b3a-cfd1-4996-b456-9fc35fcd16e1","outputId":"88f6dc14-6c43-41e2-d258-458f77d6cdfe","tags":[]},"outputs":[],"source":["# En los ejes transformados, los datos se parecen a un elipsóide alineado\n","ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n","\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Plot\n","ax.plot(*X_pca.T, marker='x', color='k', ls='')\n","ax.set_aspect('equal')\n","\n","ax.axhline(0.0, color='gray', ls=':')\n","\n","# Nombres de los ejes\n","ax.set_xlabel('$z_1$', fontsize=16)\n","ax.set_ylabel('$z_2$', fontsize=16, rotation=0)"]},{"cell_type":"markdown","id":"c43e0555-cf36-4332-a744-926216d02ec0","metadata":{"id":"c43e0555-cf36-4332-a744-926216d02ec0"},"source":["**Pregunta**. Volviendo a los ejemplos de uso de arriba (altura versus peso, etc.). ¿Pueden decir qué es lo que cada nueva coordenada $z$ representa?\n"]},{"cell_type":"markdown","id":"af71848c-a693-45d6-87a1-8756093517c4","metadata":{"id":"af71848c-a693-45d6-87a1-8756093517c4"},"source":["Podemos volver facilmente al espacio original usando el método `inverse_transform`. **Complete el código abajo con el input correcto para ese método***. Note que el código chequea si la reconstrucción se ha hecho correctamente."]},{"cell_type":"code","execution_count":null,"id":"e1c5d190-bca2-460a-b421-c17775c9ad05","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298,"status":"ok","timestamp":1661206253759,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"e1c5d190-bca2-460a-b421-c17775c9ad05","outputId":"0f945134-9ea7-4c3a-f3cf-d966875b5a9d"},"outputs":[],"source":["X_reconstructed = pca.inverse_transform(X_pca)\n","\n","print('La reconstruccuión está perfecta? {}'.format('Sí' if np.allclose(X_scaled, X_reconstructed) else 'No'))"]},{"cell_type":"markdown","id":"9c4b75a5-55ec-46e3-a782-38e1c7de95f4","metadata":{"id":"9c4b75a5-55ec-46e3-a782-38e1c7de95f4"},"source":["**Pregunta**. ¿Perdimos alguna información al ir al espacio de PCA y volver? ¿Hemos reducido la dimensionalidad del conjunto de datos?"]},{"cell_type":"markdown","id":"b425ceea-7788-4b6d-861f-678222fc328a","metadata":{"id":"b425ceea-7788-4b6d-861f-678222fc328a"},"source":["### Reconstrucción incompleta"]},{"cell_type":"markdown","id":"physical-grove","metadata":{"id":"physical-grove"},"source":["Ir al espacio PCA y volver no es una tarea de reducción de dimensionalidad y, por lo tanto, no conlleva a ninguna pérdida de información. Veamos qué ocurre si solo mantenemos la proyección de los datos en la primera componente principal."]},{"cell_type":"code","execution_count":null,"id":"binding-trail","metadata":{"id":"binding-trail"},"outputs":[],"source":["# Ejecutar PCA en un solo componente\n","pca = PCA(n_components=1)\n","X_pca_1D = pca.fit_transform(X_scaled)"]},{"cell_type":"markdown","id":"hearing-danger","metadata":{"id":"hearing-danger"},"source":["Si hacemos la transformada inversa, veremos que alguna información se ha perdido, de forma que los datos invertidos no son los mismos que los originales:"]},{"cell_type":"code","execution_count":null,"id":"bottom-necklace","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1661206306097,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"bottom-necklace","outputId":"dff55cf2-ac77-4ab5-d475-d90dca705334"},"outputs":[],"source":["X_reconstructed = pca.inverse_transform(X_pca_1D)\n","print('La reconstrucción es perfecta? {}'.format('Sí' if np.allclose(X_scaled, X_reconstructed) else 'No'))"]},{"cell_type":"markdown","id":"electrical-valve","metadata":{"id":"electrical-valve"},"source":["Miremos a esos datos comprimidos:"]},{"cell_type":"code","execution_count":null,"id":"developmental-letter","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1661206316972,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"developmental-letter","outputId":"1109f1b2-1be5-4c3b-bbcf-a32410721702"},"outputs":[],"source":["ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n","ax.plot(*X_scaled.T, marker='x', color='k', ls='', label='Datos originales')\n","ax.plot(*X_reconstructed.T, marker='x', color='orange', ls='', label='Reconstruídos')\n","ax.legend(fontsize=16)\n","ax.set_aspect('equal')"]},{"cell_type":"markdown","id":"polyphonic-place","metadata":{"id":"polyphonic-place"},"source":["**Esto** es exactamente la proyección en la dirección definida por el primer componente principal. De esa forma, el conjunto resultante comprimido retiene la mayor varianza posible. \n","\n","Si miramos a la matriz de covarianza, veremos que no se ha perdido mucho:"]},{"cell_type":"code","execution_count":null,"id":"chubby-cookbook","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1661206361640,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"chubby-cookbook","outputId":"b5182375-7c6d-4f73-93cd-afc7f76116e1"},"outputs":[],"source":["print('Matriz de covarianza original\\n', np.cov(X_scaled.T))\n","print('\\nMatriz de covarianza comprimida\\n', np.cov(X_reconstructed.T))"]},{"cell_type":"markdown","id":"unique-reasoning","metadata":{"id":"unique-reasoning"},"source":["La fracción de la varianza (información) total retenida por cada componente principal es almacenada en el atributo `explained variance_ratio_`:"]},{"cell_type":"code","execution_count":null,"id":"needed-contribution","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1661206426208,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"needed-contribution","outputId":"11f46584-6321-4d23-d406-0e287e942be9"},"outputs":[],"source":["pca.explained_variance_ratio_"]},{"cell_type":"markdown","id":"6fe1d5a9-88df-485e-905b-7bc104202bd3","metadata":{"id":"6fe1d5a9-88df-485e-905b-7bc104202bd3"},"source":["Eso muestra que la proyección en la dirección de la primer componente principal retiene más de 92% de la varianza de los datos, mientras que se ha reducido la dimensión de los datos por la mitad!"]},{"cell_type":"markdown","id":"remarkable-asthma","metadata":{"id":"remarkable-asthma","tags":[]},"source":["### *Un ejemplo en 4D (el conjunto de datos Iris)"]},{"cell_type":"markdown","id":"terminal-loading","metadata":{"id":"terminal-loading"},"source":["Para reducir la dimensionalidad, podemos elegir quedarnos con las primeras `n` componentes principales de nuestros datos. De esa forma, podemos asegurarnos que estamos tirando a la basura los componentes que cargan la menor información (según lo medido por la varianza).\n","\n","Si queremos mantener un número fijo de componentes, podemos simplemente elegir el parámetro  `n_components` de la clase `PCA` de scikit-learn. \n","\n","Usemos el conjunto de datos de Iris (una variedad de flores). Ese conjunto de datos famoso (posee su propia [página en wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)!) consiste de 50 muestras de cada una de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). \n","Cuatro características han sido medidas para cada muestra: el largo y el ancho de los sépalos y de los pétalos (en centímetros). Basándose en la combinación de esas cuatro características, este conjunto de datos es usado para aprender a clasificar las especies. Pero aquí lo usaremos simplemente para mostrar como PCA funciona en várias dimensiones."]},{"cell_type":"code","execution_count":null,"id":"nonprofit-courage","metadata":{"id":"nonprofit-courage"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","\n","data = load_iris()\n","X, t = data['data'], data['target']\n","names = data['feature_names']"]},{"cell_type":"markdown","id":"younger-proxy","metadata":{"id":"younger-proxy"},"source":["Antes de transformar los datos, hagamos gráficas de cada par de características:"]},{"cell_type":"code","execution_count":null,"id":"celtic-greenhouse","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":1455,"status":"ok","timestamp":1661199581328,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"celtic-greenhouse","outputId":"7920bf51-ea71-4096-a015-f7753a75072a"},"outputs":[],"source":["fig, axs = plt.subplots(2,3)\n","\n","for ax,(i,j) in zip(axs.flatten(),[(0,1),(2,3),(0,2),(1,3),(0,3), (1,2)]):\n","    ax.scatter(X[:,i], X[:,j],c=t,)\n","    ax.set_xlabel(names[i])\n","    ax.set_ylabel(names[j])\n","    \n","fig.set_size_inches(10,5)\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"marked-delta","metadata":{"id":"marked-delta"},"source":["Reduzcamos los datos a solo dos dimensiones. **Completen el código abajo**"]},{"cell_type":"code","execution_count":null,"id":"enclosed-religious","metadata":{"id":"enclosed-religious"},"outputs":[],"source":["pca = PCA(n_components=2)\n","X2D = pca.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"id":"engaging-creativity","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1661199699101,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"engaging-creativity","outputId":"133b8fc6-9bc3-42f2-bc9a-cc0407d547ae"},"outputs":[],"source":["print(X.shape, X2D.shape)"]},{"cell_type":"markdown","id":"gorgeous-aluminum","metadata":{"id":"gorgeous-aluminum"},"source":["Podemos usar el hecho de que estamos reteniendo solamente dos dimensiones, para hacer una gráfica del conjunto _completo_ de datos transformados. **Nota**: los colores se usan para mostrar las clases de Iris, pero el algoritmo de PCA ignora completamente esa clasificación."]},{"cell_type":"code","execution_count":null,"id":"third-colony","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1661199727287,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"third-colony","outputId":"7e10b31b-14f8-4454-a072-a3df61f583b7"},"outputs":[],"source":["plt.scatter(*X2D.T, c=t)"]},{"cell_type":"code","execution_count":null,"id":"seeing-courtesy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1661199739206,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"seeing-courtesy","outputId":"4caf66f7-9f2a-4451-c027-7b4c88633a5c"},"outputs":[],"source":["pca.explained_variance_ratio_"]},{"cell_type":"markdown","id":"mighty-sweden","metadata":{"id":"mighty-sweden"},"source":["Vemos que ahora los datos parecen más separados en la primera dirección, ¡que conserva más del 92% de la varianza de los datos!"]},{"cell_type":"markdown","id":"restricted-reason","metadata":{"id":"restricted-reason"},"source":["Podemos invertir la transformación, pero como antes alguna información se pierde en la proyección:"]},{"cell_type":"code","execution_count":null,"id":"neutral-centre","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1661199765422,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"neutral-centre","outputId":"84d7612a-0fa2-4e4b-dcc7-7fbea4db9921"},"outputs":[],"source":["X4D_inv = pca.inverse_transform(X2D)\n","print('La reconstrucción es perfecta? {}'.format('Sí' if np.allclose(X, X4D_inv) else 'No'))"]},{"cell_type":"code","execution_count":null,"id":"buried-meter","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":1344,"status":"ok","timestamp":1661199773884,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"buried-meter","outputId":"ac0ee393-20b3-4a5d-bbd2-5e84855fa4ba"},"outputs":[],"source":["# Hagamos una gráfica de las cuatro características después de la reconstrucción. Compare el resultado con la primer gráfica de esta sección.\n","fig, axs = plt.subplots(2,3)\n","\n","for ax,(i,j) in zip(axs.flatten(),[(0,1),(2,3),(0,2),(1,3),(0,3), (1,2)]):\n","    ax.scatter(X4D_inv[:,i], X4D_inv[:,j],c=t,)\n","    ax.set_xlabel(names[i])\n","    ax.set_ylabel(names[j])\n","    \n","fig.set_size_inches(10,5)\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"blank-indianapolis","metadata":{"id":"blank-indianapolis"},"source":["Una medida de cuanto se pierde de información es el *error de la reconstrucción*"]},{"cell_type":"code","execution_count":null,"id":"eligible-sweden","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1661199810795,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"eligible-sweden","outputId":"3f63dfde-cf1c-4ec8-e4a0-ba3b15cb3ecd"},"outputs":[],"source":["np.mean(np.sum(np.square(X4D_inv - X), axis=1))"]},{"cell_type":"markdown","id":"8a0f22b4-1452-44ec-a001-f86695fff43d","metadata":{"id":"8a0f22b4-1452-44ec-a001-f86695fff43d","tags":[]},"source":["### Aplicación a un conjunto de datos de gran dimensionalidad"]},{"cell_type":"markdown","id":"1724eb2d-a1cb-4771-b6e4-f3e5b031b12c","metadata":{"id":"1724eb2d-a1cb-4771-b6e4-f3e5b031b12c"},"source":["Encontrar una representación de menor dimensión de nuestros datos nos permite guardarlos usando menos espacio y resontruirlos permitiendo perder la menor información posible. Veamos como eso funciona con imágenes:"]},{"cell_type":"code","execution_count":null,"id":"4fa1f982-8305-4acf-9135-fb900dc392eb","metadata":{"id":"4fa1f982-8305-4acf-9135-fb900dc392eb"},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","mnist.target = mnist.target.astype(np.uint8)\n","\n","X_mnist = mnist[\"data\"]/255.0 # vean la normalización\n","t_mnist = mnist[\"target\"]"]},{"cell_type":"code","execution_count":null,"id":"b3819d27","metadata":{},"outputs":[],"source":["np.unique(t_mnist)"]},{"cell_type":"code","execution_count":null,"id":"1544d03b-887f-45ac-b689-c7cce470e51a","metadata":{"id":"1544d03b-887f-45ac-b689-c7cce470e51a"},"outputs":[],"source":["def plot_digits(instances, images_per_row=5, **options):\n","    size = 28\n","    images_per_row = min(len(instances), images_per_row)\n","    images = [instance.reshape(size,size) for instance in instances]\n","    n_rows = (len(instances) - 1) // images_per_row + 1\n","    row_images = []\n","    n_empty = n_rows * images_per_row - len(instances)\n","    images.append(np.zeros((size, size * n_empty)))\n","    for row in range(n_rows):\n","        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n","        row_images.append(np.concatenate(rimages, axis=1))\n","    image = np.concatenate(row_images, axis=0)\n","    plt.imshow(image, cmap = plt.cm.binary, **options)\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"id":"1818bb39-fb44-4992-b38e-7ae5b0050e5f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":683},"id":"1818bb39-fb44-4992-b38e-7ae5b0050e5f","outputId":"37d137b2-036b-4e68-b10b-5e1246454eaa"},"outputs":[],"source":["fig =  plt.figure(figsize=(12,12))\n","plot_digits(X_mnist[:100], 10)"]},{"cell_type":"markdown","id":"850d66ca-2c66-4a15-9caa-32f9c42c1861","metadata":{"id":"850d66ca-2c66-4a15-9caa-32f9c42c1861"},"source":["Cada dígito consiste de 784 pixeles. Veamos cómo quedan las imágenes reconstruídas cuando usamos PCA para guardarlas en un vector de menor dimensionalidad.\n","\n","**Completen el código abajo con un valor razonable para el número de componentes**"]},{"cell_type":"code","execution_count":null,"id":"c97bd598-7b0a-4e7f-b34d-a7e9c30904aa","metadata":{"id":"c97bd598-7b0a-4e7f-b34d-a7e9c30904aa"},"outputs":[],"source":["pca = PCA(n_components=154)\n","\n","X_reduced = pca.fit_transform(X_mnist)\n","X_recovered = pca.inverse_transform(X_reduced)"]},{"cell_type":"code","execution_count":null,"id":"85007c5c-fafe-404a-a61b-34f5f7afb327","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1661200053080,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"85007c5c-fafe-404a-a61b-34f5f7afb327","outputId":"0de1274d-b6e1-4183-b5a0-31e33ac4f3b4"},"outputs":[],"source":["print(X_mnist.shape, X_reduced.shape, X_recovered.shape)"]},{"cell_type":"code","execution_count":null,"id":"eb8dd392-73d7-4c7b-9279-b639bdd55fd1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1661200057955,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"eb8dd392-73d7-4c7b-9279-b639bdd55fd1","outputId":"8e3d32c3-4242-4c31-c90b-3ab2aa46a9f3"},"outputs":[],"source":["plt.figure(figsize=(12, 7))\n","plt.subplot(121)\n","plot_digits(X_mnist[::2100])\n","plt.title(\"Original\", fontsize=16)\n","plt.subplot(122)\n","plot_digits(np.abs(X_recovered[::2100]))\n","plt.title(\"Comprimido\", fontsize=16)"]},{"cell_type":"markdown","id":"facfff3c-1ae1-4d9a-831a-e54ae30def2c","metadata":{"id":"facfff3c-1ae1-4d9a-831a-e54ae30def2c"},"source":["Si no estamos seguros sobre qué dimensionalidad deberíamos imponer, podemos hacer una gráfica de la suma cumulativa de la razón de la varianza explicada para ver cuanta información se pierde. Si todas las características son usadas, la suma tendría que ser 1."]},{"cell_type":"code","execution_count":null,"id":"c8c13ff4-d8a3-45e6-baf8-ef80ffe54d38","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13660,"status":"ok","timestamp":1661200122894,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"c8c13ff4-d8a3-45e6-baf8-ef80ffe54d38","outputId":"c6e91daf-7db0-4188-b19c-1526d5be311f"},"outputs":[],"source":["pca = PCA()\n","pca.fit(X_mnist[t_mnist==3]) #fiteo sin reducir la dimensionalidad"]},{"cell_type":"code","execution_count":null,"id":"053b7cee-6cc5-4ba4-a062-21b4f81f2e72","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1661200136148,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"053b7cee-6cc5-4ba4-a062-21b4f81f2e72","outputId":"b18f85fc-5f05-4a47-f2a8-b60b7bf92d4a"},"outputs":[],"source":["# Ver los primeros elementos de la variancia explicada \n","print(pca.explained_variance_ratio_[:10])"]},{"cell_type":"markdown","id":"1b6b6902-0e9a-48e8-8000-95b047dd86f2","metadata":{"id":"1b6b6902-0e9a-48e8-8000-95b047dd86f2"},"source":["**Pregunta**. Compare y contraste con las clases arriba (el conjunto de datos Iris y el conjunto de datos simulado). ¿Qué puede notar?"]},{"cell_type":"markdown","id":"c1392870-30a0-4517-916b-a304e3849e2c","metadata":{"id":"c1392870-30a0-4517-916b-a304e3849e2c"},"source":["Es interesante obtener el número de componentes que necesitamos mantener para preservar un dado nivel de la varianza. Por ejemplo, 95%. Para eso, podemos calcular la *suma cumulativa* de las razones de la varianza explicada."]},{"cell_type":"code","execution_count":null,"id":"66b53a17-746d-4a2a-8e03-588c9d111980","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1661200297676,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"66b53a17-746d-4a2a-8e03-588c9d111980","outputId":"b0b5e379-e133-41de-9cab-fad9fc92873f"},"outputs":[],"source":["# Definir qué fracción de la varianza se quiere mantener\n","var_frac = 0.95\n","\n","# Calcular la suma cumulativa y hacer su gráfica\n","cumsum = np.cumsum(pca.explained_variance_ratio_) \n","# eso nos dice cuanta información es retenida si paramos en cada dimensión\n","\n","# En qué momento la suma cumulativa llega a var_frac * 100 %?\n","d = np.argmax(cumsum >= var_frac) + 1\n","print('Con {} dimensiones, preservamos el {} de la varianza.'.format(d, var_frac))\n","\n","plt.figure(figsize=(8,5))\n","plt.plot(cumsum, linewidth=3)\n","# plt.axis([0, 400, 0, 1])\n","\n","plt.axvline(d, color=\"k\", ls=\":\")\n","plt.plot([0, d], [0.95, 0.95], \"k:\")\n","plt.plot(d, 0.95, \"ko\")\n","\n","plt.xlabel(\"Dimensiones\", fontsize=16)\n","plt.ylabel(\"Varianza Explicada\", fontsize=16)\n","\n","plt.grid(True)"]},{"cell_type":"markdown","id":"a75c4808-601b-4b13-b486-4fbf2b020ffc","metadata":{"id":"a75c4808-601b-4b13-b486-4fbf2b020ffc","tags":[]},"source":["### Visualizando las Componentes Principales"]},{"cell_type":"markdown","id":"d359e084-02f6-4e99-856e-4ea5a46e480f","metadata":{"id":"d359e084-02f6-4e99-856e-4ea5a46e480f"},"source":["A pesar de que la tarea con la cual estamos lidiando ahora es la reducción de dimensionalidad, las direcciones dadas por los componentes principales también se pueden pensar como instancias individuales en el espacio de los datos, que corresponde cada una a una imágen de 28 x 28 pixeles.\n","\n","Nos podemos preguntar cómo se ven los primeros componentes principales para este conjunto de datos."]},{"cell_type":"code","execution_count":null,"id":"a1cf24cc-2f20-4987-928f-a46b0582fd41","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4148,"status":"ok","timestamp":1661200341660,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"a1cf24cc-2f20-4987-928f-a46b0582fd41","outputId":"a2b89494-9b99-4f10-d946-3e5a60d4e8ac"},"outputs":[],"source":["npc = 100\n","\n","ncolumns = 10\n","nrows = npc // ncolumns\n","\n","# Agregar una línear extra, si se necesita\n","if npc % ncolumns:\n","    nrows += 1\n","\n","fig, axs = plt.subplots(nrows, ncolumns, figsize=(16, 2*nrows))\n","\n","for i, ax in zip(range(npc), axs.flatten()):\n","    pci_reshaped = pca.components_[i].reshape(28,28)\n","    ax.imshow(pci_reshaped, cmap='bwr_r')\n","    ax.axis('off')"]},{"cell_type":"markdown","id":"0f3d2589-55a1-40b2-8c0a-44643b80aeb5","metadata":{"id":"0f3d2589-55a1-40b2-8c0a-44643b80aeb5"},"source":["**Pregunta.** ¿Pueden notar algo que valga le pena mencionar a partir de esas figuras?\n","\n","Noten como el tipo de imágen cambia cuando nos movemos de las primeras pocas CP hacia las de órdenes más altas. ¿Qué creen que significa eso?"]},{"cell_type":"code","execution_count":null,"id":"fd21483d-3a45-40af-8946-ee64e0092733","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1661200434864,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"fd21483d-3a45-40af-8946-ee64e0092733","outputId":"53fd5812-4cdd-4210-91b5-f2e01c5f9ef7"},"outputs":[],"source":["# Podemos hacer una gráfica del promedio\n","plt.imshow(pca.mean_.reshape(28, 28), cmap='gray')"]},{"cell_type":"markdown","id":"pQVQpIwEcq4W","metadata":{"id":"pQVQpIwEcq4W"},"source":["Podemos ver la distribución de los pesos de cada componente principal para 4 dígitos diferentes:"]},{"cell_type":"code","execution_count":null,"id":"90d4b75f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":509},"executionInfo":{"elapsed":1577,"status":"ok","timestamp":1661200449027,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"90d4b75f","outputId":"7f0d2777-8137-455b-cf25-9170930704ee"},"outputs":[],"source":["fig = plt.figure(figsize=(12,8))\n","\n","for i in [10, 20, 30, 40]:\n","    # Plotear los coeficientes de los componentes\n","    plt.loglog(np.abs(pca.transform(X_mnist[i].reshape(1, -1))).flatten(), \n","               label=t_mnist[i], alpha=0.8)\n","    \n","    # Plotear los pixeles originales\n","    # plt.plot(np.abs((X_mnist[i].reshape(1, -1))).flatten(), \n","    #         label=t_mnist[i], alpha=0.4)\n","# plt.xlim(0, 10)\n","plt.legend(loc=0)"]},{"cell_type":"code","execution_count":null,"id":"24beff49","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":964,"status":"ok","timestamp":1661200470073,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"24beff49","outputId":"60fef3ab-ee2a-43ea-d74f-7ccd5e259a3f"},"outputs":[],"source":["X_mnist.shape"]},{"cell_type":"markdown","id":"expensive-exposure","metadata":{"heading_collapsed":"true","id":"expensive-exposure","jp-MarkdownHeadingCollapsed":true,"tags":[],"toc-hr-collapsed":true},"source":["## *Kernel PCA"]},{"cell_type":"markdown","id":"textile-credit","metadata":{"id":"textile-credit"},"source":["Una variante del PCA que se usa para aprender mapeos no-lineales es el llamado *kernel-PCA*, que usa el *truco del kernel* para hacer un mapeo de las características originales en un espacio de dimensionalidad mayor (quizás dimensionalidad infinita), en el cual el PCA se aplica. Eso es útil para aprender transformaciones no-lineales.\n","\n","El método de Kernel PCA tiene la **gran** desventaja de que la transformación inversa no es fácil de obtener. Los invitamos a explorar este algoritmo y experimentar con sus distintos hyperparámetros:\n","\n","* n_components\n","* kernel. Elegir cual _kernel_ usar para hacer el mapeo a un espacio de dimensionalidad más grande. Pueden intentar: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’} \n","* gamma. Escala de tamaño para el _kernel_ 'rbf'\n","* degree. Grado del polinómio para el _kernel_ 'poly'"]},{"cell_type":"code","execution_count":null,"id":"loose-comedy","metadata":{"id":"loose-comedy"},"outputs":[],"source":["from sklearn.decomposition import KernelPCA\n","\n","rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)"]},{"cell_type":"markdown","id":"05d309fc-3bdf-4d8a-8d2e-9476de67fce3","metadata":{"id":"05d309fc-3bdf-4d8a-8d2e-9476de67fce3"},"source":["### Conjunto de datos Iris"]},{"cell_type":"code","execution_count":null,"id":"da70f2b0-c555-4756-a1a9-9cf568d77aab","metadata":{"id":"da70f2b0-c555-4756-a1a9-9cf568d77aab"},"outputs":[],"source":["X_kpca = rbf_pca.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"id":"precise-procurement","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"precise-procurement","outputId":"5e8adc24-aafc-46fe-ceac-46996697eaa0"},"outputs":[],"source":["plt.scatter(*X_kpca.T)"]},{"cell_type":"markdown","id":"basic-aquatic","metadata":{"id":"basic-aquatic"},"source":["Veamos como eso se comporta con un conjunto de datos más complejos:"]},{"cell_type":"markdown","id":"3f35f836-4a89-47c5-9da7-ad81ccb606ea","metadata":{"id":"3f35f836-4a89-47c5-9da7-ad81ccb606ea","tags":[]},"source":["### El pionono (_Swiss Roll_)"]},{"cell_type":"code","execution_count":null,"id":"colonial-simple","metadata":{"id":"colonial-simple"},"outputs":[],"source":["from sklearn.datasets import make_swiss_roll\n","\n","X, z = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"]},{"cell_type":"markdown","id":"entire-canon","metadata":{"id":"entire-canon"},"source":["Este conjunto de datos es un plano enrollado, que es más complicado *decomponer*"]},{"cell_type":"code","execution_count":null,"id":"fiscal-checkout","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"fiscal-checkout","outputId":"8f8f96f9-8dfc-4559-f851-a928bf207223"},"outputs":[],"source":["plt.scatter(X[:, 1], X[:, 2], c=z,  cmap=plt.cm.hot)"]},{"cell_type":"code","execution_count":null,"id":"committed-eugene","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"committed-eugene","outputId":"38512d2d-9be3-4c2c-851c-c424016c80d9"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","fig = plt.figure(figsize=(15,15))\n","\n","ax = fig.add_subplot(211, projection='3d')\n","ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=z, cmap=plt.cm.hot)\n","ax.view_init(10, -70)"]},{"cell_type":"markdown","id":"noticed-restaurant","metadata":{"id":"noticed-restaurant"},"source":["Comparemos los resultados de distintos _kernels_"]},{"cell_type":"code","execution_count":null,"id":"massive-uniform","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"massive-uniform","outputId":"356d94cd-4b09-49f2-8c29-2fe0dd757fff"},"outputs":[],"source":["lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True) #equivalente a PCA(n_components=2)\n","rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n","sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n","\n","\n","plt.figure(figsize=(11, 4))\n","for subplot, pca, title in ((131, lin_pca, \"Kernel lineal\"), \n","                            (132, rbf_pca, \"Kernel RBF, $\\gamma=0.04$\"), \n","                            (133, sig_pca, \"Kernel sigmoide , $\\gamma=10^{-3}, r=1$\")):\n","    X_reduced = pca.fit_transform(X)\n","    if subplot == 132:\n","        X_reduced_rbf = X_reduced\n","    \n","    plt.subplot(subplot)\n","    plt.title(title, fontsize=14)\n","    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=z, cmap=plt.cm.hot)\n","    plt.xlabel(\"$z_1$\", fontsize=18)\n","    if subplot == 131:\n","        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n","    plt.grid(True)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"artistic-recommendation","metadata":{"id":"artistic-recommendation","tags":[]},"source":["## Aprendizaje de la Variedad (_Manifold Learning_)"]},{"cell_type":"markdown","id":"61cbd619-0f20-46c6-b74c-6432a7079f7c","metadata":{"id":"61cbd619-0f20-46c6-b74c-6432a7079f7c"},"source":["El aprendizaje de variedades se usa para descubrir patrones no-lineales en los datos.\n","\n","Para explorar eso, vamos a usar el conjunto de datos _pionono_ (en inglés _Swiss roll_)."]},{"cell_type":"code","execution_count":null,"id":"0b82c32a-93ed-448c-a256-cc88bf727291","metadata":{"id":"0b82c32a-93ed-448c-a256-cc88bf727291"},"outputs":[],"source":["from sklearn.datasets import make_swiss_roll\n","\n","X_sr, z = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"]},{"cell_type":"markdown","id":"71a54cd3-53b4-4126-85c6-37d800588e95","metadata":{"id":"71a54cd3-53b4-4126-85c6-37d800588e95"},"source":["Este conjunto de datos es un plano enrollado, que es más complicado *descomponer*"]},{"cell_type":"code","execution_count":null,"id":"de12a50f","metadata":{},"outputs":[],"source":["%matplotlib widget\n"]},{"cell_type":"code","execution_count":null,"id":"eef1b3bb-f140-4230-a27e-ff476e02ea66","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"eef1b3bb-f140-4230-a27e-ff476e02ea66","outputId":"bd437c2e-8fa7-4c93-cfdc-06676cc5b975"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","fig = plt.figure(figsize=(15,15))\n","\n","ax = fig.add_subplot(211, projection='3d')\n","ax.scatter(X_sr[:, 0], X_sr[:, 1], X_sr[:, 2], c=z, cmap=plt.cm.hot)\n","ax.view_init(10, -70)\n","plt.show()"]},{"cell_type":"markdown","id":"unusual-composition","metadata":{"id":"unusual-composition"},"source":["Evidentemente, la proyección óptima para el pionono sería algo que _desenrolla_ los datos. Como estos datos los armamos a mano, podemos desenrollarlo perfectamente, pero esto no va a pasar en general con otros conjuntos de datos."]},{"cell_type":"code","execution_count":null,"id":"durable-reader","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"elapsed":608,"status":"ok","timestamp":1661200648546,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"durable-reader","outputId":"1c9a7fc1-d6fb-4dfd-94b5-8771141704e0"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","plt.scatter(z, X_sr[:,1], c=z,  cmap=plt.cm.hot)"]},{"cell_type":"markdown","id":"da93afdd-0403-4379-8871-fcff907230ee","metadata":{"id":"da93afdd-0403-4379-8871-fcff907230ee"},"source":["Aprender ese tipo de subespacios, que están inmersos en un espacio de mayor dimensionalidad se llama *aprendizaje de varieadades* (*Manifold Learning*). "]},{"cell_type":"markdown","id":"af355960-f77c-4c49-a844-b4984c67078e","metadata":{"id":"af355960-f77c-4c49-a844-b4984c67078e"},"source":["### Local Linear Embedding"]},{"cell_type":"markdown","id":"aa47ccc1-81d0-41b3-a32d-1b6e0d3c8fb8","metadata":{"id":"aa47ccc1-81d0-41b3-a32d-1b6e0d3c8fb8"},"source":["Un método simple, que no cubriremos en detalles es el de *Inmersión Localmente Lineal* (*Locally Linear Embedding*, LLE). \n","\n","LLE funciona ajustando un hiperplano en `n_components` dimensiones a los `n_neighbors` puntos de cada instancia. Después proyecta los datos a esos subespacios fiteados. Veamos como funciona con el pionono.\n","\n","Como siempre, instanciamos el objeto con algunos hiperparámetros y usamos el método `fit_transform`"]},{"cell_type":"code","execution_count":null,"id":"characteristic-ottawa","metadata":{"id":"characteristic-ottawa"},"outputs":[],"source":["from sklearn.manifold import LocallyLinearEmbedding\n","\n","lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n","X_reduced = lle.fit_transform(X_sr)"]},{"cell_type":"code","execution_count":null,"id":"5d2b8360-1be9-4b67-8e45-1d356202fb4e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1661200772391,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"5d2b8360-1be9-4b67-8e45-1d356202fb4e","outputId":"01e41075-aac7-4b3b-b761-6f4381d36e3f"},"outputs":[],"source":["# Verificar las dimensiones de lo que hemos encontrado\n","print(X_sr.shape, X_reduced.shape)"]},{"cell_type":"code","execution_count":null,"id":"historic-nirvana","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":308},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1661200779493,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"historic-nirvana","outputId":"5ead709f-d7df-4453-f62c-1c038e431084"},"outputs":[],"source":["# Hacer una gráfica de los resultados\n","plt.title(\"Pionono desenrollado usando LLE\", fontsize=14)\n","plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=z, cmap=plt.cm.hot)\n","plt.xlabel(\"$z_1$\", fontsize=18)\n","plt.ylabel(\"$z_2$\", fontsize=18)\n","# plt.axis([-0.065, 0.055, -0.1, 0.12])\n","plt.grid(True)"]},{"cell_type":"markdown","id":"vital-agency","metadata":{"id":"vital-agency"},"source":["**No** está mal, pero tampoco es óptimo. \n","\n","**Tarea**. Experimentar con el hyperparámetros `n_neighbors`. ¿Quieren resultados mejores? Navegue por la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) y vea si consigue encontrar algo más para ajustar."]},{"cell_type":"markdown","id":"85bb29bd-2c3b-4b90-86b9-a37b705c2b3e","metadata":{"id":"85bb29bd-2c3b-4b90-86b9-a37b705c2b3e","tags":[]},"source":["### tSNE"]},{"cell_type":"markdown","id":"1751c113-35d0-423e-b360-8d75fd992c02","metadata":{"id":"1751c113-35d0-423e-b360-8d75fd992c02"},"source":["Una técnica popular de aprendizaje de variedades usada para visualizaciones es el de *Inmersión de vecinos estocástica t-distribuída* (*t-Distributed Stochastic Neighbor Embedding*,  tSNE). Esa técnica aprende un mapeo no lineal que tiende a agrupar instancias similares, mientras separa instancias disimilares.\n","\n","Vamos a probarla en el conjunto de datos MNIST. ¡Vean que simple es usarlo! Como siempre, se crea una instancia, se usa `fit_transform` y ¡a disfrutar!"]},{"cell_type":"code","execution_count":null,"id":"swedish-secretariat","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7682,"status":"ok","timestamp":1661200830384,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"swedish-secretariat","outputId":"0fa29369-12ac-414e-e808-af10fa27eb48"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","tsne = TSNE(n_components=2, random_state=42)\n","\n","X_reduced_tsne = tsne.fit_transform(X_mnist[:1000]) #fitea un subconjunto para disminuir el tiempo de cómputo"]},{"cell_type":"code","execution_count":null,"id":"c34eca90","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1661200847822,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"c34eca90","outputId":"43149248-8ee8-41ee-ae1f-f08e82b30878"},"outputs":[],"source":["t_mnist.shape"]},{"cell_type":"code","execution_count":null,"id":"entitled-criminal","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"elapsed":1509,"status":"ok","timestamp":1661200854126,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"entitled-criminal","outputId":"e483977d-99ba-48d0-ad32-a3ff0220d06e"},"outputs":[],"source":["fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111)\n","scat = ax.scatter(*X_reduced_tsne.T, c=t_mnist[:1000], s=50, cmap='jet', \n","                  edgecolors='None', alpha=0.8)\n","fig.colorbar(scat)"]},{"cell_type":"code","execution_count":null,"id":"c6989f96","metadata":{},"outputs":[],"source":["i_raros = (X_reduced_tsne[:, 0] >20) & (t_mnist[:1000] > 1)\n","xx = X_mnist[:1000][i_raros][0]\n","\n","plt.imshow(xx.reshape(28,28), cmap=plt.cm.binary)\n","\n"]},{"cell_type":"markdown","id":"1c4ee181-3f0d-4d96-9827-1cb9fc94c0da","metadata":{"id":"1c4ee181-3f0d-4d96-9827-1cb9fc94c0da"},"source":["Los números se han agrupado en algunos casos, aunque el algoritmo sea totalmente ignorante de los dígitos reales asociados a cada dato (que han sido usados para colorear los puntos). ¡Increíble!"]},{"cell_type":"markdown","id":"0003f71d-37de-4962-975d-622d8aee04d1","metadata":{"id":"0003f71d-37de-4962-975d-622d8aee04d1"},"source":["Ahora intentemos eso en 3D"]},{"cell_type":"code","execution_count":null,"id":"1c2d9aed-6b78-478d-8c90-ab6315072299","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28542,"status":"ok","timestamp":1661200903661,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"1c2d9aed-6b78-478d-8c90-ab6315072299","outputId":"64322c42-cfd0-4c5f-cb84-a617b2d9255e"},"outputs":[],"source":["tsne = TSNE(n_components=3, random_state=42)\n","X_reduced_tsne_3d = tsne.fit_transform(X_mnist[:1000])#fit a subset to reduce computing time"]},{"cell_type":"code","execution_count":null,"id":"4e3dcbfe-b30e-4844-811c-dc1fc1cb9191","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"elapsed":1284,"status":"ok","timestamp":1661200906789,"user":{"displayName":"Martin Makler","userId":"07988031307934820444"},"user_tz":180},"id":"4e3dcbfe-b30e-4844-811c-dc1fc1cb9191","outputId":"7dafbb24-dbca-4b87-d4a9-ce4e6d29aa8d"},"outputs":[],"source":["fig = plt.figure(figsize=(12, 10))\n","ax = fig.add_subplot(projection='3d')\n","\n","scat = ax.scatter(*X_reduced_tsne_3d.T, c=t_mnist[:1000], s=10, cmap='jet', \n","                   alpha=0.8)\n","\n","ax.set_xlim(-15, 15)\n","ax.set_ylim(-15, 15)\n","fig.colorbar(scat)"]}],"metadata":{"colab":{"collapsed_sections":["potential-diamond","c7a7be40-6431-434a-a555-6bbdd1c0074c","77c69347","dcbca033","82ec60dd-80ae-4611-94f7-e12148757c19","30f53729-b076-44b4-a0fc-fa0496b96b50","930087ca-95d1-4780-b4af-e37f41966834","b425ceea-7788-4b6d-861f-678222fc328a","remarkable-asthma","8a0f22b4-1452-44ec-a001-f86695fff43d","a75c4808-601b-4b13-b486-4fbf2b020ffc","expensive-exposure","05d309fc-3bdf-4d8a-8d2e-9476de67fce3","3f35f836-4a89-47c5-9da7-ad81ccb606ea"],"name":"Notebook_03_reduccion_dimensional.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.13 ('tf-mac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"toc-autonumbering":true,"toc-showcode":false,"toc-showmarkdowntxt":false,"toc-showtags":true,"vscode":{"interpreter":{"hash":"2b7aa682480b82eb27ca7b5ecfebdb0027bb2a276e6bdff64c1ddeab03557e9e"}}},"nbformat":4,"nbformat_minor":5}
