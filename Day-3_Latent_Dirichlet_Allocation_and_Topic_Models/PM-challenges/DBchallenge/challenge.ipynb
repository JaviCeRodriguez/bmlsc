{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from dbConnect import singleQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docs: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize news, removing no wished chars (numers, for example).\n",
    "\n",
    "    Pre: body of news as list of strings\n",
    "    Post: return a list of lists that represents each body of news \n",
    "    \"\"\"\n",
    "    #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[áéíóúÁÉÍÓÚñÑüÜ]+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def lemmatize(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def lemmatize_es(docs):\n",
    "    # https://spacy.io/usage/models\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs:\n",
    "        lemmatized_doc = [token.lemma_ for token in nlp(\" \".join(doc))]\n",
    "        lemmatized_docs.append(lemmatized_doc)\n",
    "    return lemmatized_docs\n",
    "\n",
    "\n",
    "def ngrams(docs):\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def bag_of_words(docs):\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.8)\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return dictionary, corpus\n",
    "\n",
    "\n",
    "def train_lda(dictionary, corpus):\n",
    "    # Set training parameters.\n",
    "    num_topics = 10\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 20\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    top_topics = model.top_topics(corpus)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "    pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = None\n",
    "\n",
    "with open('cnn.txt', 'rt', encoding='utf8') as file:\n",
    "    spa = [x.strip() for x in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 0\n",
      "Number of documents: 38\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of unique tokens: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(dictionary))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of documents: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_lda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 70\u001b[0m, in \u001b[0;36mtrain_lda\u001b[1;34m(dictionary, corpus)\u001b[0m\n\u001b[0;32m     67\u001b[0m eval_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Don't evaluate model perplexity, takes too much time.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Make an index to word dictionary.\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mdictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# This is only to \"load\" the dictionary.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m id2word \u001b[38;5;241m=\u001b[39m dictionary\u001b[38;5;241m.\u001b[39mid2token\n\u001b[0;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m LdaModel(\n\u001b[0;32m     74\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[0;32m     75\u001b[0m     id2word\u001b[38;5;241m=\u001b[39mid2word,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     eval_every\u001b[38;5;241m=\u001b[39meval_every\n\u001b[0;32m     83\u001b[0m )\n",
      "File \u001b[1;32m~\\Documents\\UNSAM\\bmlsc\\venvw\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:107\u001b[0m, in \u001b[0;36mDictionary.__getitem__\u001b[1;34m(self, tokenid)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# the word->id mapping has changed (presumably via add_documents);\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# recompute id->word accordingly\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mrevdict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid2token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenid\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "bodies = tokenize(spa)\n",
    "bodies = lemmatize(bodies)\n",
    "bodies = ngrams(bodies)\n",
    "dictionary, corpus = bag_of_words(bodies)\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "train_lda(dictionary, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = singleQuery('SELECT titulo,cuerpo,fuente FROM noticias LIMIT 5000;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8304\n",
      "Number of documents: 5000\n",
      "Average topic coherence: -0.9590.\n",
      "[([(0.039896023, 'yo'),\n",
      "   (0.015275833, 'hacer'),\n",
      "   (0.01390909, 'todo'),\n",
      "   (0.013580222, 'mucho'),\n",
      "   (0.013564318, 'ese'),\n",
      "   (0.012565902, 'decir'),\n",
      "   (0.01188011, 'ir'),\n",
      "   (0.011872036, 'mi'),\n",
      "   (0.01164261, 'pero'),\n",
      "   (0.008752707, 'porque'),\n",
      "   (0.008735408, 'tú'),\n",
      "   (0.00850746, 'más'),\n",
      "   (0.007437887, 'cuando'),\n",
      "   (0.007169367, 'poder'),\n",
      "   (0.0067767184, 'ver'),\n",
      "   (0.006546261, 'querer'),\n",
      "   (0.006243128, 'si'),\n",
      "   (0.006062277, 'año'),\n",
      "   (0.0059497813, 'saber'),\n",
      "   (0.0059002656, 'pasar')],\n",
      "  -0.5531276223687694),\n",
      " ([(0.014508255, 'más'),\n",
      "   (0.0130036045, 'poder'),\n",
      "   (0.008436715, 'año'),\n",
      "   (0.007897847, 'mucho'),\n",
      "   (0.0075886957, 'otro'),\n",
      "   (0.007163707, 'todo'),\n",
      "   (0.0071431547, 'pero'),\n",
      "   (0.006974974, 'ese'),\n",
      "   (0.006793707, 'hacer'),\n",
      "   (0.0065933596, 'también'),\n",
      "   (0.004400888, 'entre'),\n",
      "   (0.0043065627, 'alguno'),\n",
      "   (0.004202656, 'sobre'),\n",
      "   (0.0040117605, 'si'),\n",
      "   (0.0038929202, 'vez'),\n",
      "   (0.0038612362, 'mundo'),\n",
      "   (0.0038015256, 'persona'),\n",
      "   (0.0037681593, 'nuevo'),\n",
      "   (0.003543851, 'primero'),\n",
      "   (0.003463227, 'cada')],\n",
      "  -0.6161099943267939),\n",
      " ([(0.010429211, 'partido'),\n",
      "   (0.01026604, 'más'),\n",
      "   (0.008693663, 'equipo'),\n",
      "   (0.008656428, 'jugar'),\n",
      "   (0.008532789, 'primero'),\n",
      "   (0.007711729, 'pero'),\n",
      "   (0.006501421, 'ese'),\n",
      "   (0.0064414046, 'año'),\n",
      "   (0.0063854023, 'final'),\n",
      "   (0.0057942187, 'dos'),\n",
      "   (0.0056080683, 'ante'),\n",
      "   (0.0055726618, 'poder'),\n",
      "   (0.005506756, 'argentino'),\n",
      "   (0.0053040083, 'hacer'),\n",
      "   (0.005128354, 'jugador'),\n",
      "   (0.0050007594, 'mundial'),\n",
      "   (0.00484063, 'desde'),\n",
      "   (0.004834335, 'ganar'),\n",
      "   (0.004794969, 'todo'),\n",
      "   (0.0044869636, 'mucho')],\n",
      "  -0.7489381799362377),\n",
      " ([(0.008604813, 'poder'),\n",
      "   (0.008123579, 'ese'),\n",
      "   (0.0069769155, 'decir'),\n",
      "   (0.006120511, 'año'),\n",
      "   (0.005880525, 'sobre'),\n",
      "   (0.005133029, 'contra'),\n",
      "   (0.005093764, 'otro'),\n",
      "   (0.0050084325, 'presidente'),\n",
      "   (0.004989045, 'político'),\n",
      "   (0.004977861, 'caso'),\n",
      "   (0.0049591307, 'justicia'),\n",
      "   (0.004691525, 'hacer'),\n",
      "   (0.004524589, 'todo'),\n",
      "   (0.004426945, 'público'),\n",
      "   (0.0044161608, 'gobierno'),\n",
      "   (0.004322836, 'ley'),\n",
      "   (0.0042075193, 'pero'),\n",
      "   (0.004193207, 'ex'),\n",
      "   (0.0040799533, 'más'),\n",
      "   (0.0040028496, 'deber')],\n",
      "  -0.9069409010462339),\n",
      " ([(0.009951982, 'candidato'),\n",
      "   (0.008314653, 'más'),\n",
      "   (0.008081611, 'milei'),\n",
      "   (0.0070520123, 'massa'),\n",
      "   (0.0065356274, 'junto'),\n",
      "   (0.0064696586, 'cambio'),\n",
      "   (0.006303572, 'hacer'),\n",
      "   (0.0061523863, 'ese'),\n",
      "   (0.006075989, 'todo'),\n",
      "   (0.0060555455, 'gobernador'),\n",
      "   (0.00603872, 'ir'),\n",
      "   (0.00595252, 'elección'),\n",
      "   (0.00555182, 'bullrich'),\n",
      "   (0.0054205665, 'gobierno'),\n",
      "   (0.0052832956, 'provincia'),\n",
      "   (0.005194352, 'poder'),\n",
      "   (0.004882238, 'frente'),\n",
      "   (0.004719306, 'electoral'),\n",
      "   (0.0043796236, 'presidencial'),\n",
      "   (0.0043377657, 'si')],\n",
      "  -0.9453330322263949),\n",
      " ([(0.01903966, 'dólar'),\n",
      "   (0.011291028, 'más'),\n",
      "   (0.007579685, 'us'),\n",
      "   (0.0070297867, 'blue'),\n",
      "   (0.006900113, 'agua'),\n",
      "   (0.0063986145, 'oficial'),\n",
      "   (0.0063970457, 'desde'),\n",
      "   (0.0063447566, 'millón'),\n",
      "   (0.0062342617, 'día'),\n",
      "   (0.0058699884, 'us_millón'),\n",
      "   (0.0050580967, 'también'),\n",
      "   (0.005002212, 'entre'),\n",
      "   (0.0048800055, 'dólar_blue'),\n",
      "   (0.0048329765, 'otro'),\n",
      "   (0.0048224437, 'paso'),\n",
      "   (0.004809657, 'hasta'),\n",
      "   (0.004716141, 'venta'),\n",
      "   (0.0044524116, 'ir'),\n",
      "   (0.004397728, 'poder'),\n",
      "   (0.0043432694, 'semana')],\n",
      "  -1.0402824196731253),\n",
      " ([(0.011866016, 'san'),\n",
      "   (0.010200495, 'argentino'),\n",
      "   (0.009036827, 'aires'),\n",
      "   (0.008793521, 'buenos'),\n",
      "   (0.008489148, 'ciudad'),\n",
      "   (0.008233409, 'buenos_aires'),\n",
      "   (0.008030266, 'desde'),\n",
      "   (0.0075963214, 'nacional'),\n",
      "   (0.00718538, 'entre'),\n",
      "   (0.006308473, 'año'),\n",
      "   (0.005107074, 'juan'),\n",
      "   (0.0050358335, 'zona'),\n",
      "   (0.004791439, 'argentina'),\n",
      "   (0.0046176063, 'río'),\n",
      "   (0.0045953267, 'martín'),\n",
      "   (0.00455955, 'país'),\n",
      "   (0.0043883463, 'local'),\n",
      "   (0.004379475, 'dos'),\n",
      "   (0.004328983, 'provincia'),\n",
      "   (0.004267187, 'donde')],\n",
      "  -1.0981031064492788),\n",
      " ([(0.007920791, 'policía'),\n",
      "   (0.0068673096, 'dos'),\n",
      "   (0.0060513834, 'según'),\n",
      "   (0.0059798397, 'casa'),\n",
      "   (0.0059190164, 'ese'),\n",
      "   (0.0058361683, 'año'),\n",
      "   (0.0057301256, 'otro'),\n",
      "   (0.0054431614, 'mujer'),\n",
      "   (0.005128387, 'donde'),\n",
      "   (0.00488319, 'víctima'),\n",
      "   (0.0047392063, 'sena'),\n",
      "   (0.004645022, 'cecilia'),\n",
      "   (0.0045960313, 'quien'),\n",
      "   (0.0044979546, 'cuando'),\n",
      "   (0.0043760655, 'encontrar'),\n",
      "   (0.004356616, 'joven'),\n",
      "   (0.0043490822, 'fiscal'),\n",
      "   (0.004265974, 'familia'),\n",
      "   (0.004195932, 'persona'),\n",
      "   (0.0041855215, 'también')],\n",
      "  -1.1726911421619435),\n",
      " ([(0.0111138215, 'israel'),\n",
      "   (0.0095051695, 'decir'),\n",
      "   (0.009260585, 'más'),\n",
      "   (0.0073984936, 'israelí'),\n",
      "   (0.0072610104, 'guerra'),\n",
      "   (0.007156151, 'ataque'),\n",
      "   (0.006381059, 'gaza'),\n",
      "   (0.0057992763, 'país'),\n",
      "   (0.005372885, 'desde'),\n",
      "   (0.005222728, 'poder'),\n",
      "   (0.0051880865, 'ruso'),\n",
      "   (0.0050645075, 'entre'),\n",
      "   (0.0050143762, 'ucrania'),\n",
      "   (0.00491209, 'militar'),\n",
      "   (0.004811412, 'pero'),\n",
      "   (0.004709811, 'según'),\n",
      "   (0.0046832846, 'hama'),\n",
      "   (0.00458488, 'palestino'),\n",
      "   (0.0043403977, 'fuerza'),\n",
      "   (0.0040810173, 'contra')],\n",
      "  -1.2506311971816975),\n",
      " ([(0.013340501, 'más'),\n",
      "   (0.010361251, 'poder'),\n",
      "   (0.006706311, 'tu'),\n",
      "   (0.006364004, 'ese'),\n",
      "   (0.0061106533, 'año'),\n",
      "   (0.0059832926, 'mes'),\n",
      "   (0.005579897, 'todo'),\n",
      "   (0.005481282, 'si'),\n",
      "   (0.0051893364, 'precio'),\n",
      "   (0.0048938743, 'nuevo'),\n",
      "   (0.0047261217, 'millón'),\n",
      "   (0.004643574, 'otro'),\n",
      "   (0.004626083, 'hacer'),\n",
      "   (0.004431038, 'empresa'),\n",
      "   (0.004426006, 'sin'),\n",
      "   (0.004220638, 'dinero'),\n",
      "   (0.004141397, 'inflación'),\n",
      "   (0.0040814523, 'hoy'),\n",
      "   (0.004030977, 'salud'),\n",
      "   (0.003823972, 'mercado')],\n",
      "  -1.2581296443133108)]\n"
     ]
    }
   ],
   "source": [
    "bodies = [body for _, body, _ in noticias]\n",
    "bodies = tokenize(bodies)\n",
    "bodies = lemmatize_es(bodies)\n",
    "bodies = ngrams(bodies)\n",
    "dictionary, corpus = bag_of_words(bodies)\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "train_lda(dictionary, corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
