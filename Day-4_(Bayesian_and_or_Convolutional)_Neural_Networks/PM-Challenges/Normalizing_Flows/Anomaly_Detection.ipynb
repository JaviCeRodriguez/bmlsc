{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414c1ae7",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "In this notebook we'll introduce two types of anomaly detection and learn how to perform such analyses using Normalizing Flows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python stuff\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stuff for torch+nflows\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "# optimizer from torch\n",
    "from torch import optim\n",
    "\n",
    "# base Flow to construct model\n",
    "from nflows.flows.base import Flow\n",
    "#base distribution to use\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "# the MADE coupling layer\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "# this adds a RandomPermutation to add variance between layers\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "# this will combine modules\n",
    "from nflows.transforms.base import CompositeTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b10641",
   "metadata": {},
   "source": [
    "# Anomaly Detection:\n",
    "\n",
    "Anomaly detection is in some sense self-explanatory: given a dataset $X$ we want to find a subset $X'$ which is \"anomalous\" or different. This could be to detect malicious outliers: a spam filter, a banking fraud detector, simply badly measured samples, etc. Or it could be to detect gold: high-reward stocks or options, good fits for a sporting team, etc.\n",
    "\n",
    "The task will define is the anomaly is good or bad. Additionally, it may define the type of anomaly we seek. There are roughly two types of anomalies: out-of-density and over-densities. Usually, anomaly detection is an **unsupervised** task, where we do not have labels to train our models and simply try to understand the data and find anomalies within. Some methods are **semi-supervised** because they use some noisy labels to get a better sense of what an anomaly is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e814da",
   "metadata": {},
   "source": [
    "## Out-of-density\n",
    "\n",
    "In out-of-density cases, we are really looking for some outliers, things that are far away from most of the data. In 1D, this is very easy to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c237dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "X1 = st.norm().rvs(int(0.99*N))\n",
    "X2 = st.norm(loc=10,scale=0.1).rvs(int(0.01*N))\n",
    "X=np.hstack([X1,X2])\n",
    "plt.hist(X,histtype='step')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb76243",
   "metadata": {},
   "source": [
    "You can see how there is a small subset of data which is far away from the rest. This would be our anomaly. Again, we do not have labels here. You might say that this is very easy: just look at the data and that's it. However, 1D is very misleading. As you increase the number of dimensions, you not only lose visualization but *every* point is in some sense far away from the rest. This is **the curse of dimensionality**.\n",
    "\n",
    "However, we can use Normalizing Flows to our advantange here. We can simply define the outliers as the points with lowest probability. Thus, our **anomaly score** is simply Log $p(X)$. We can use our anomaly score to select events in the usual way:\n",
    "\n",
    "Anomalous events ($\\alpha$) = {$x$ | Log $p(x) \\leq \\alpha$ }\n",
    "\n",
    "Where $\\alpha$ is the parametric choice that selects how anomalous do we want our anomalous events to be. You can think that $\\alpha$ defines events whose probability are lower or equal to $e^\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3cede",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Given the 2D dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=50000\n",
    "X1 = st.multivariate_normal(mean=[1.5,-1.5],cov=[[1,0.1],[0.1,0.5]]).rvs(int(0.999*N))\n",
    "X2 = st.multivariate_normal(mean=[-1.,1.],cov=[[0.7,-0.2],[-0.2,1.2]]).rvs(int(0.001*N))\n",
    "X=np.vstack([X1,X2])\n",
    "Y=np.hstack([np.ones(len(X1)),-np.ones(len(X2))])\n",
    "print(X.shape)\n",
    "plt.scatter(X[:,0],X[:,1],c=Y,alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d5650",
   "metadata": {},
   "source": [
    "Train a Normalizing Flow: this includes implementing a batch-size to deal with the higher number of events (not done in the previous notebook, but can be done simply by sampling a subset of X at each iteration) and more importantly **evaluating** whether the flow has trained succesfully or not. An interesting question here is whether we need to flow to match the exact dataset or just the bulk of the dataset so we get anomalous events. The degree of precision depends on the application.\n",
    "\n",
    "Use the anomaly score to select \"intereting\" events. Produce summary plots. Some suggestions: A useful metric (which would not be available in real data) is the fraction of selected events of X2 as a function of a $\\alpha$. Another is a scatter plot as above but where the color is the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f99ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
