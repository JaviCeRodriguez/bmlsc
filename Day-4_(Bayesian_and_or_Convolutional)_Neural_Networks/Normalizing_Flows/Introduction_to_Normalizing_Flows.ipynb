{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e2fda",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we'll see what are Normalizing Flows exactly and play a bit with a standard implementation. Let's import what we need. We need to have [`pytorch`](https://pytorch.org/get-started/locally/) and [`nflows`](https://github.com/bayesiains/nflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python stuff\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stuff for torch+nflows\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal, ConditionalDiagonalNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.nn.nets import ResidualNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81e96c",
   "metadata": {},
   "source": [
    "# Transformation rules of probability density functions\n",
    "\n",
    "The transformation rule of pdfs for a change of variable $x=f(z)$ is\n",
    "\n",
    "$p_{X}(x)=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{x}f^{-1}\\left(x\\right)|$\n",
    "\n",
    "The determinant of the Jacobian can be rewritten in terms of $f$ for ease of computation as\n",
    "\n",
    "$p_{X}(x)=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{x}f^{-1}\\left(x\\right)|=p_{Z}\\left(f^{-1}(x)\\right)|\\text{det} \\nabla_{z}f\\left(f^{-1}(x)\\right)|^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108224e",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "Show how you can transform a normally distributed $x\\sim \\mathcal{N}(0,1)$ to a normally distributed variable $x\\sim \\mathcal{N}(\\mu,\\sigma)$ through the change of variables $x = \\mu + \\sigma z$ by completing this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "### an example of this\n",
    "N = 10000\n",
    "pdf_z = st.norm(loc=0,scale=1)\n",
    "Z = pdf_z.rvs(N)\n",
    "plt.hist(Z,histtype='step',density=True)\n",
    "ztoplot = np.linspace(-5,5,100) # dummy variables for plotting the pdf\n",
    "pdf_vals_z = pdf_z.pdf(ztoplot) # evalute pdf for plotting\n",
    "plt.plot(ztoplot,pdf_vals_z)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7871051",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= 1.0 # arbitrary values\n",
    "sigma= 0.5 # arbitrary values\n",
    "X = mu+sigma*zvals\n",
    "plt.hist(X,histtype='step',density=True)\n",
    "xtoplot = mu+sigma*ztoplot # dummy variables for plotting the pdf\n",
    "gradient_xtoplot_over_z = # compute the gradient\n",
    "pdf_vals_x = pdf_z.pdf(ztoplot) *  ... # evaluate the new pdf using the old pdf + jacobian\n",
    "plt.plot(xtoplot,pdf_vals_x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bf9d6",
   "metadata": {},
   "source": [
    "\n",
    "# Normalizing Flows\n",
    "\n",
    "At its essence, Normalizing Flows are bijective functions that map a sample space to a new space where data is distributed however we chose it to. That is, if we have data $x\\sim p_{X}$, we want to learn an invertible function $x = f(z,\\theta)$ such that $z$ follows an base distribution easy to sample from and to evaluate. The most common choice is a normal distribution $z \\sim p_{Z}\\equiv \\mathcal{N}(0,1)$. \n",
    "\n",
    "$f$ will be a learnable neural network with parameters $\\theta$ and an easy to compute gradient. The loss function which $\\theta$ needs to minimize is nothing more than the negative Log Likelihood obtained using the transformation rule of pdfs\n",
    "\n",
    "$\\mathcal{L}=- \\sum_{x\\in \\mathcal{D}}\\text{Ln }p_{X}(x) = - \\sum_{x\\in \\mathcal{D}}\\text{Ln }[p_{Z}(f^{-1}(x,\\theta))|\\text{det }\\nabla_{z}f|^{-1}]$\n",
    "\n",
    "$\\mathcal{L}= \\sum_{x\\in \\mathcal{D}}\\left(-\\text{Ln }[p_{Z}(f^{-1}(x,\\theta))]+\\text{Ln }[|\\text{det }\\nabla_{z}f|]\\right)$\n",
    "\n",
    "And assuming a standard normal distribution \n",
    "\n",
    "$\\mathcal{L}= \\sum_{x\\in \\mathcal{D}}\\left(-\\text{Ln }\\mathcal{N}\\left(f^{-1}(x,\\theta);0,1\\right)+\\text{Ln }[|\\text{det }\\nabla_{z}f|]\\right)$\n",
    "\n",
    "The trick is how to chose a learnable $f$ with easy gradient (which is not a problem using the gradient chain rule with standard NNs + backpropagation) but also easily invertable to go back and forth from $x$ to $z$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17473c08",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0235ff",
   "metadata": {},
   "source": [
    "In the previous, very simplified example, we know that a good choice of $f(z,\\theta)$ is simply $f(z,\\theta)=\\theta_{0}+\\theta_{1}z$ with inverse $f^{-1}(x,\\theta)=(x-\\theta_{0})/\\theta_{1}$ and jacobian $|\\text{det }\\nabla_{z}f|=|\\theta_{1}|$ (which does not depend on the evaluation on $z = (x-\\theta_{0})/\\theta_{1}$. We can thus simply write the loss function and do a very naive grid minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34563f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(theta0,theta1):\n",
    "    return np.sum(-st.norm(loc=0,scale=1).logpdf((X-theta0)/theta1)+np.log(theta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0vals = np.linspace(0.5,1.5,100) # substitute adequate range if you changed mu, sigma before\n",
    "theta1vals = np.linspace(0.3,0.7,100) # substitute adequate range if you changed mu, sigma before\n",
    "theta0vals_plot, theta1vals_plot = np.meshgrid(theta0vals,theta1vals)\n",
    "# print(theta0vals_plot.shape,theta1vals_plot.shape)\n",
    "loss_function_vals = np.zeros(theta0vals_plot.shape)\n",
    "for ntheta1val, theta1val in enumerate(theta1vals):\n",
    "    for ntheta0val, theta0val in enumerate(theta0vals):\n",
    "        loss_function_vals[ntheta1val,ntheta0val]=loss_function(theta0val,theta1val)\n",
    "plt.contourf(theta0vals,theta1vals,loss_function_vals,cmap='gist_heat_r')\n",
    "plt.axhline(sigma)\n",
    "plt.axvline(mu)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0min, theta1min = theta0vals_plot.flatten()[np.argmin(loss_function_vals)],theta1vals_plot.flatten()[np.argmin(loss_function_vals)]\n",
    "print(theta0min,theta1min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(mu,sigma),loss_function(theta0min,theta1min) # why? likely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X,histtype='step',density=True)\n",
    "# plt.plot(xtoplot,pdf_vals_x)\n",
    "# now we use the min parameters explicitly with xtoplot\n",
    "pdf_vals_x_bis = st.norm(loc=0,scale=1).pdf((xtoplot-theta0min)/theta1min)/theta1min\n",
    "plt.plot(xtoplot,pdf_vals_x_bis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17bcb8",
   "metadata": {},
   "source": [
    "Note that we **haven't seen the true Z during training**. The technique is aimed at learning $p_{X}(x)$. We did cheat by knowing that the simple parameterization was good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a012e7",
   "metadata": {},
   "source": [
    "# Choice of f\n",
    "\n",
    "There are [many](https://arxiv.org/pdf/1908.09257.pdf) ways to do this, but the usual trick consists of concatenating several individual, simpler modules. That is\n",
    "\n",
    "$z_{1} = f_{1}(z)$\n",
    "\n",
    "$z_{i} = f_{i-1}(z_{i-1})$ with $i=2,...,n-1$\n",
    "\n",
    "$x=f_{n}(z_{n-1})$\n",
    "\n",
    "and having each individual $f_{i}$ module as a simple, invertible function whose parameters are Neural Networks. For example, the very common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c01b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
